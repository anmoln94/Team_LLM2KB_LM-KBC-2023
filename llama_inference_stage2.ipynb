{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6a8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import TextStreamer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a939a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompts = pd.read_csv('question-prompts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15900251",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_lora_injection_type = 'method1' #change to method 2 if you want to load adapter trained using method 2 of paper\n",
    "\n",
    "if llama_lora_injection_type == 'method1':\n",
    "    path = \"./llama_method1_injection\" #directory of method 1 trained LoRA injection model\n",
    "else:\n",
    "    path = \"./llama_method2_injection\" #directory of method 2 trained LoRA injection model\n",
    "\n",
    "#change cache_dir path to where you have kept meta-llama/Llama-2-13b-chat-hf\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    path,\n",
    "    load_in_4bit=True,\n",
    "    cache_dir=\"./llama_base_directory\",\n",
    "    device_map='auto',\n",
    "    use_cache=False\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce84f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.0,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    streamer=streamer\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38075456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_extract_lst(corrupt_lst):\n",
    "    lst_extract_prompt = f'''<s>[INST] <<SYS>>\n",
    " Example 1: Wrong Format: ['People's Republic of China', 'Laos', 'Thailand', 'India', 'Bangladesh']\"]. Correct Format: Answer: \"People's Republic of China\", \"Laos\", \"Thailand\", \"India\", \"Bangladesh\"] </s>\n",
    " Example 2: Wrong Format: ['Artibonite', 'Nord-Est Department', 'South Department', 'West Department', 'Centre Department', 'Grand'Anse Department', 'North Department']. Correct Format: Answer: \"Artibonite\", \"Nord-Est Department\", \"South Department\", \"West Department\", \"Centre Department\", \"Grand'Anse Department\", \"North Department\"] </s>\n",
    " Example 3: Wrong Format: ['book's and page's']. Correct Format: Answer: [\"book's and page's\"] </s>\n",
    " Your answer should only be a valid python list of string format. Do not give any explainations.\n",
    " <</SYS>>\n",
    "\n",
    " Use the examples to convert {corrupt_lst} into a correct python list. [/INST] Answer: '''\n",
    "\n",
    "    with llm.pipeline.model.disable_adapter():\n",
    "        ans = llm(lst_extract_prompt)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e383c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_match_prompt_llama(sample,options):\n",
    "    return f'''<s>[INST] <<SYS>>\n",
    " You are a helpful, respectful and honest assistant. Your answers should be crisp, short and not repititive.\n",
    " Choose an answer from the options in the context.\n",
    " If you dont know the answer from the given context, answer should just be a python empty list.\n",
    " <</SYS>>\n",
    " context: {options}\n",
    " \n",
    " {question_prompts[question_prompts['Relation']==sample['Relation']]['PromptTemplate'].tolist()[0].replace('{subject_entity}',sample['SubjectEntity'])} [/INST] Answer:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bbbef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lst_regex(input_string):\n",
    "    extracted_list = []\n",
    "    pattern = r'\\[([^\\]]+)\\]'\n",
    "    matches = re.findall(pattern, input_string)\n",
    "    if matches:\n",
    "        if type(eval(matches[0])) is tuple:\n",
    "            extracted_list = list(eval(matches[0]))\n",
    "        else:\n",
    "            extracted_list = [str(eval(matches[0]))]\n",
    "    return extracted_list\n",
    "\n",
    "\n",
    "def extract_list_from_string(input_string):\n",
    "    try:\n",
    "        extracted_list = lst_regex(input_string)\n",
    "    except:\n",
    "        llm_lst = llm_extract_lst(input_string)\n",
    "        print(\"llm_lst\", llm_lst, type(llm_lst))\n",
    "        try:\n",
    "            extracted_list = lst_regex(llm_lst)\n",
    "        except:\n",
    "            extracted_list = []\n",
    "\n",
    " \n",
    "\n",
    "    return extracted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448703d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_dataframe_as_generator(df):\n",
    "    for index, row in df.iterrows():\n",
    "        yield row\n",
    "\n",
    "df = pd.read_csv(\"llama_with_wikidata_info.csv\")\n",
    "df[\"WikiTitles\"] = df[\"WikiTitles\"].apply(literal_eval) \n",
    "df[\"ObjectEntities\"] = df[\"ObjectEntities\"].apply(literal_eval) \n",
    "df_generator = iterate_dataframe_as_generator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034803a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsondata = []\n",
    "while True:\n",
    "    try:\n",
    "        row = next(df_generator)\n",
    "        OrigAnsWikiTitles = []\n",
    "        for object_options in row[\"WikiTitles\"]:\n",
    "            if object_options and len(object_options)>0 and object_options != [None]:\n",
    "                formatted_sample = best_match_prompt_llama(row, object_options)\n",
    "                res = llm(formatted_sample)\n",
    "                OrigAnsWikiTitles.append(extract_list_from_string(res))\n",
    "            else:\n",
    "                OrigAnsWikiTitles.append([])\n",
    "\n",
    "        row[\"OrigAnsWikiTitle\"] = OrigAnsWikiTitles\n",
    "        jsondata.append(row)       \n",
    "    except StopIteration:\n",
    "        # Break out of the loop when there are no more values to yield\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred for record: {single_sub_ent}\")\n",
    "        print(f\"Error message: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(jsondata).to_csv(\"llama_stage2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
