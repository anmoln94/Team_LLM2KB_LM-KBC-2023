{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad316a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from transformers import (\n",
    "    DPRContextEncoder,\n",
    "    DPRContextEncoderTokenizer,\n",
    "    DPRQuestionEncoder,\n",
    "    DPRQuestionEncoderTokenizer,\n",
    ")\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee661d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_model_path = \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    "query_model_path = \"facebook/dpr-question_encoder-single-nq-base\"\n",
    "cache_dir = \"/fs/scratch/ban_bgsw_etm-team/nyn1kor_scratch/\"\n",
    "\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(context_model_path,cache_dir=cache_dir)\n",
    "context_model = DPRContextEncoder.from_pretrained(context_model_path,cache_dir=cache_dir).eval()\n",
    "query_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(query_model_path,cache_dir=cache_dir)\n",
    "query_model = DPRQuestionEncoder.from_pretrained(query_model_path,cache_dir=cache_dir).eval()\n",
    "\n",
    "class DPREmbeddings:\n",
    "\n",
    "    def encode(self, tokenizer, model, texts, max_length=300, batch_size=64):\n",
    "        model.to(device)\n",
    "        outputs = []\n",
    "        num_texts = len(texts)\n",
    "        start_idx = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            while start_idx < num_texts:\n",
    "                end_idx = min(start_idx + batch_size, num_texts)\n",
    "                batch_texts = texts[start_idx:end_idx]\n",
    "\n",
    "                # Tokenize the batch of texts\n",
    "                inputs = tokenizer(\n",
    "                    batch_texts,\n",
    "                    padding=\"longest\",\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                    return_tensors=\"pt\",\n",
    "                ).to(device)\n",
    "\n",
    "                # Forward pass through the model\n",
    "                output = model(**inputs).pooler_output.cpu().numpy()\n",
    "                outputs.extend(output)\n",
    "\n",
    "                start_idx += batch_size\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        texts = [text.replace(\"\\n\", \" \") for text in texts]\n",
    "        pooled_outputs = self.encode(context_tokenizer, context_model, texts)\n",
    "        return pooled_outputs\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        pooled_output = self.encode(\n",
    "            query_tokenizer, query_model, [text], batch_size=1\n",
    "        )\n",
    "        return pooled_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_chunk_length=300, overlap=50):\n",
    "    # Split text into paragraphs using '\\n\\n'\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "\n",
    "    # Split each paragraph into words\n",
    "    paragraphs = [paragraph.split() for paragraph in paragraphs]\n",
    "\n",
    "    # Split each paragraph into chunks with overlap at word level using list comprehension\n",
    "    split_text_result = [paragraph[i:i + max_chunk_length] for paragraph in paragraphs\n",
    "                         for i in range(0, len(paragraph), max_chunk_length - overlap)]\n",
    "\n",
    "    # Join the words back into paragraphs\n",
    "    split_text_result = [' '.join(chunk) for chunk in split_text_result]\n",
    "\n",
    "    return split_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f78239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_contexts(q, wiki_txt,k):\n",
    "    if wiki_txt:\n",
    "        wiki_chunked = split_text(wiki_txt)\n",
    "        vectorstore = FAISS.from_texts(wiki_chunked, DPREmbeddings())\n",
    "        top_contexts = vectorstore.similarity_search(q, k=k)\n",
    "        top_contexts = [d.page_content for d in top_contexts]\n",
    "        return \"\\n\".join(top_contexts)\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompts = pd.read_csv('question-prompts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a6254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('traindata_with_context.jsonl','r') as trainfile:\n",
    "    train_json_list = list(trainfile)\n",
    "with open('valdata_with_context.jsonl','r') as valfile:\n",
    "    val_json_list = list(valfile)    \n",
    "with open('testdata_with_context.jsonl','r') as testfile:\n",
    "    test_json_list = list(testfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed198da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_list=[]\n",
    "val_df_list=[]\n",
    "test_df_list=[]\n",
    "\n",
    "for i,json_str in enumerate(train_json_list):\n",
    "    result = json.loads(json_str)     \n",
    "    train_df_list.append(result)\n",
    "for i,json_str in enumerate(val_json_list):\n",
    "    result = json.loads(json_str)     \n",
    "    val_df_list.append(result)\n",
    "for i,json_str in enumerate(test_json_list):\n",
    "    result = json.loads(json_str)     \n",
    "    test_df_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e542aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_df_list)\n",
    "train_df = train_df.sample(frac = 1,random_state=8)\n",
    "\n",
    "val_df = pd.DataFrame(val_df_list)\n",
    "val_df = val_df.sample(frac = 1,random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556fc9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction_llama(sample):\n",
    "    return f'''<s>[INST] <<SYS>>\n",
    " You are a helpful, respectful and honest assistant. Your answers should be crisp, short and not repititive.\n",
    " Give valid wikipedia page titles in the answer.\n",
    " <</SYS>>\n",
    " {question_prompts[question_prompts['Relation']==sample['Relation']]['PromptTemplate'].tolist()[0].replace('{subject_entity}',sample['SubjectEntity'])} [/INST] Answer: {sample['ObjectEntities']} </s>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6858717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question(sample):\n",
    "    return f'''{question_prompts[question_prompts['Relation']==sample['Relation']]['PromptTemplate'].tolist()[0].replace('{subject_entity}',sample['SubjectEntity'])}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be60939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_with_context_llama(sample,k):\n",
    "    context = get_k_contexts(sample['question'], sample['subject_text'],k)\n",
    "    return f'''<s>[INST] <<SYS>>\n",
    " You are a helpful, respectful and honest assistant. Your answers should be crisp, short and not repititive.\n",
    " Give valid wikipedia page titles in the answer. The answer should be in a python list of string format.\n",
    " If you dont know the answer from both the given context and your past knowledge, answer should just be a python empty list.\n",
    " <</SYS>>\n",
    " context: '{context}'\n",
    "    \n",
    " {sample['question']} [/INST] Answer: {sample['ObjectEntities']} </s>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_without_context_llama(sample):\n",
    "    return f'''<s>[INST] <<SYS>>\n",
    " You are a helpful, respectful and honest assistant. Your answers should be crisp, short and not repititive.\n",
    " Give valid wikipedia page titles in the answer. The answer should be in a python list of string format.\n",
    " If you dont know the answer from both the given context and your past knowledge, answer should just be a python empty list.\n",
    " <</SYS>>\n",
    " context: ''\n",
    "    \n",
    " {sample['question']} [/INST] Answer: {sample['ObjectEntities']} </s>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_match_prompt_llama(query,options,answer):\n",
    "    return f'''<s>[INST] <<SYS>>\n",
    " You are a helpful, respectful and honest assistant. Your answers should be crisp, short and not repititive.\n",
    " If you dont know the answer from the given context, answer should just be a python empty list.\n",
    " <</SYS>>\n",
    " context: {options}\n",
    " \n",
    " Which among the context options is equivalent to {query}? [/INST] Answer: ['{answer}'] </s>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04980c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_match_prompt_beluga(sample,options,answer):\n",
    "    return f'''### System:\n",
    "You are a helpful, respectful and honest assistant. Your answers should be crisp, short and not repititive.\n",
    "Choose an answer from the options in the context.\n",
    "If you dont know the answer from the given context, answer should just be a python empty list.\n",
    "\n",
    "### User:\n",
    "context: {options}\n",
    "\n",
    "{question_prompts[question_prompts['Relation']==sample['Relation']]['PromptTemplate'].tolist()[0].replace('{subject_entity}',sample['SubjectEntity'])}\n",
    "\n",
    "### Assistant\n",
    "Answer: {answer}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_with_context_beluga(sample):\n",
    "    context = get_k_contexts(sample['question'], sample['subject_text'])\n",
    "    return f'''### System:\n",
    "You are a helpful, respectful and honest assistant. Your answers should be crisp, short and not repititive.\n",
    "Give valid wikipedia page titles in the answer. The answer should be in a python list of string format.\n",
    "If you dont know the answer from both the given context and your past knowledge, answer should just be a python empty list.\n",
    "\n",
    "### User:\n",
    "context: '{context}'\n",
    "\n",
    "{sample['question']}\n",
    "\n",
    "### Assistant\n",
    "Answer: {sample['ObjectEntities']}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87861b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_without_context_beluga(sample):\n",
    "    context = get_k_contexts(sample['question'], sample['subject_text'])\n",
    "    return f'''### System:\n",
    "You are a helpful, respectful and honest assistant. Your answers should be crisp, short and not repititive.\n",
    "Give valid wikipedia page titles in the answer. The answer should be in a python list of string format.\n",
    "If you dont know the answer from both the given context and your past knowledge, answer should just be a python empty list.\n",
    "\n",
    "### User:\n",
    "context: ''\n",
    "\n",
    "{sample['question']}\n",
    "\n",
    "### Assistant\n",
    "Answer: {sample['ObjectEntities']}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eb82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'llama' #change to beluga if generating data for beluga model\n",
    "\n",
    "train_df['question']=train_df.apply(lambda x: get_question(x),axis=1)\n",
    "val_df['question']=val_df.apply(lambda x: get_question(x),axis=1)\n",
    "\n",
    "if model =='llama':\n",
    "    train_df['prompt_with_context']=train_df.apply(lambda x: get_prompt_with_context_llama(x,2), axis=1)\n",
    "    val_df['prompt_with_context']=val_df.apply(lambda x: get_prompt_with_context_llama(x,2), axis=1)\n",
    "    train_df.to_csv('train_with_top2_context_llama.csv')\n",
    "    val_df.to_csv('val_with_top2_context_llama.csv')\n",
    "    \n",
    "    train_df['prompt_with_context']=train_df.apply(lambda x: get_prompt_without_context_llama(x,2), axis=1)\n",
    "    val_df['prompt_with_context']=val_df.apply(lambda x: get_prompt_without_context_llama(x,2), axis=1)\n",
    "    train_df.to_csv('train_without_context_llama.csv')\n",
    "    val_df.to_csv('val_without_context_llama.csv')\n",
    "    \n",
    "    train_df_just_with_prompt = pd.DataFrame(columns=['prompt_with_context'])\n",
    "    df = pd.read_csv('train_df_with_candidates.csv') #this file is generated by running fetch wiki options notebook with train.jsonl\n",
    "    count=0\n",
    "    for row in df.iterrows():\n",
    "        queries = literal_eval(row[1]['ObjectEntities'])\n",
    "        options = literal_eval(row[1]['WikiTitles'])\n",
    "        answers = literal_eval(row[1]['OrigAnsWikiTitle'])\n",
    "        if len(queries)!= len(answers) or len(queries)!=len(options):\n",
    "            count+=1\n",
    "            continue\n",
    "        else:\n",
    "            for i, query in enumerate(queries):\n",
    "                if query.strip()!='':\n",
    "                    train_df_just_with_prompt.loc[len(train_df_just_with_prompt.index)] = [best_match_prompt_llama(query,str(options[i]),answers[i])]\n",
    "\n",
    "    train_df_just_with_prompt.to_csv('train_with_wiki_candidates_llama.csv')\n",
    "\n",
    "    val_df_just_with_prompt = pd.DataFrame(columns=['prompt_with_context'])\n",
    "    df = pd.read_csv('val_df_with_candidates.csv') #this file is generated by running fetch wiki options notebook with val.jsonl\n",
    "    count=0\n",
    "    for row in df.iterrows():\n",
    "        queries = literal_eval(row[1]['ObjectEntities'])\n",
    "        options = literal_eval(row[1]['WikiTitles'])\n",
    "        answers = literal_eval(row[1]['OrigAnsWikiTitle'])\n",
    "        if len(queries)!= len(answers) or len(queries)!=len(options):\n",
    "            count+=1\n",
    "            continue\n",
    "        else:\n",
    "            for i, query in enumerate(queries):\n",
    "                if query.strip()!='':\n",
    "                    val_df_just_with_prompt.loc[len(val_df_just_with_prompt.index)] = [best_match_prompt_llama(query,str(options[i]),answers[i])]\n",
    "\n",
    "    val_df_just_with_prompt.to_csv('val_with_wiki_candidates_llama.csv')\n",
    "    \n",
    "    train_df_with_context = pd.read_csv('train_with_top2_context_llama.csv')\n",
    "    train_df_without_context = pd.read_csv('train_without_context_llama.csv')\n",
    "    train_df_with_wiki = pd.read_csv('train_with_wiki_candidates_llama.csv')\n",
    "\n",
    "    val_df_with_context = pd.read_csv('val_with_top2_context_llama.csv')\n",
    "    val_df_without_context = pd.read_csv('val_without_context_llama.csv')\n",
    "    val_df_with_wiki = pd.read_csv('val_with_wiki_candidates_llama.csv')\n",
    "\n",
    "    train_df_final = pd.concat([train_df_with_context,train_df_without_context,train_df_with_wiki])\n",
    "    val_df_final = pd.concat([val_df_with_context,val_df_without_context,val_df_with_wiki])\n",
    "    \n",
    "else:\n",
    "    train_df['prompt_with_context']=train_df.apply(lambda x: get_prompt_with_context_beluga(x,2), axis=1)\n",
    "    val_df['prompt_with_context']=val_df.apply(lambda x: get_prompt_with_context_beluga(x,2), axis=1)\n",
    "    train_df.to_csv('train_with_top2_context_beluga.csv')\n",
    "    val_df.to_csv('val_with_top2_context_beluga.csv')\n",
    "    \n",
    "    train_df['prompt_with_context']=train_df.apply(lambda x: get_prompt_without_context_beluga(x,2), axis=1)\n",
    "    val_df['prompt_with_context']=val_df.apply(lambda x: get_prompt_without_context_beluga(x,2), axis=1)\n",
    "    train_df.to_csv('train_without_context_beluga.csv')\n",
    "    val_df.to_csv('val_without_context_beluga.csv')\n",
    "    \n",
    "    train_df_just_with_prompt = pd.DataFrame(columns=['prompt_with_context'])\n",
    "    df = pd.read_csv('train_df_with_candidates.csv') #this file is generated by running fetch wiki options notebook with train.jsonl\n",
    "    count=0\n",
    "    for row in df.iterrows():\n",
    "        queries = literal_eval(row[1]['ObjectEntities'])\n",
    "        options = literal_eval(row[1]['WikiTitles'])\n",
    "        answers = literal_eval(row[1]['OrigAnsWikiTitle'])\n",
    "        if len(queries)!= len(answers) or len(queries)!=len(options):\n",
    "            count+=1\n",
    "            continue\n",
    "        else:\n",
    "            for i, query in enumerate(queries):\n",
    "                if query.strip()!='':\n",
    "                    train_df_just_with_prompt.loc[len(train_df_just_with_prompt.index)] = [best_match_prompt_beluga(query,str(options[i]),answers[i])]\n",
    "\n",
    "    train_df_just_with_prompt.to_csv('train_with_wiki_candidates_beluga.csv')\n",
    "\n",
    "    val_df_just_with_prompt = pd.DataFrame(columns=['prompt_with_context'])\n",
    "    df = pd.read_csv('val_df_with_candidates.csv') #this file is generated by running fetch wiki options notebook with val.jsonl\n",
    "    count=0\n",
    "    for row in df.iterrows():\n",
    "        queries = literal_eval(row[1]['ObjectEntities'])\n",
    "        options = literal_eval(row[1]['WikiTitles'])\n",
    "        answers = literal_eval(row[1]['OrigAnsWikiTitle'])\n",
    "        if len(queries)!= len(answers) or len(queries)!=len(options):\n",
    "            count+=1\n",
    "            continue\n",
    "        else:\n",
    "            for i, query in enumerate(queries):\n",
    "                if query.strip()!='':\n",
    "                    val_df_just_with_prompt.loc[len(val_df_just_with_prompt.index)] = [best_match_prompt_beluga(query,str(options[i]),answers[i])]\n",
    "\n",
    "    val_df_just_with_prompt.to_csv('val_with_wiki_candidates_beluga.csv')\n",
    "    \n",
    "    train_df_with_context = pd.read_csv('train_with_top2_context_beluga.csv')\n",
    "    train_df_without_context = pd.read_csv('train_without_context_beluga.csv')\n",
    "    train_df_with_wiki = pd.read_csv('train_with_wiki_candidates_beluga.csv')\n",
    "\n",
    "    val_df_with_context = pd.read_csv('val_with_top2_context_beluga.csv')\n",
    "    val_df_without_context = pd.read_csv('val_without_context_beluga.csv')\n",
    "    val_df_with_wiki = pd.read_csv('val_with_wiki_candidates_beluga.csv')\n",
    "\n",
    "    train_df_final = pd.concat([train_df_with_context,train_df_without_context,train_df_with_wiki])\n",
    "    val_df_final = pd.concat([val_df_with_context,val_df_without_context,val_df_with_wiki])\n",
    "    \n",
    "train_df_prompt = pd.DataFrame(columns=['prompt'])\n",
    "val_df_prompt = pd.DataFrame(columns=['prompt'])\n",
    "train_df_prompt['prompt'] = train_df_final['prompt_with_context']\n",
    "val_df_prompt['prompt'] = val_df_final['prompt_with_context']\n",
    "train_df_prompt.to_csv('train_df_combined.csv')\n",
    "val_df_prompt.to_csv('val_df_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae54dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.iterrows():\n",
    "    queries = literal_eval(row[1]['ObjectEntities'])\n",
    "    options = literal_eval(row[1]['WikiTitles'])\n",
    "    answers = literal_eval(row[1]['OrigAnsWikiTitle'])\n",
    "    if len(queries)!= len(answers) or len(queries)!=len(options):\n",
    "        count+=1\n",
    "        continue\n",
    "    else:\n",
    "        for i, query in enumerate(queries):\n",
    "            if query.strip()!='':\n",
    "                df_just_with_prompt.loc[len(df_just_with_prompt.index)] = [best_match_prompt(query,str(options[i]),answers[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2826d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_with_top3_context.jsonl','w') as outfile:\n",
    "    for json_str in test_json_list:\n",
    "        sample = json.loads(json_str)\n",
    "        question=question_prompts[question_prompts['Relation']==sample['Relation']]['PromptTemplate'].tolist()[0].replace('{subject_entity}',sample['SubjectEntity'])\n",
    "        sample['top3context'] = get_k_contexts(question,sample['subject_text'],3)\n",
    "        del sample['subject_text']\n",
    "        json.dump(sample,outfile)\n",
    "        outfile.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
