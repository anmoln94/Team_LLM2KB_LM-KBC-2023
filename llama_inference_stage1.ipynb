{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f451da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import TextStreamer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b9cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompts = pd.read_csv('question-prompts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a2256",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_lora_injection_type = 'method1' #change to method 2 if you want to load adapter trained using method 2 of paper\n",
    "\n",
    "if llama_lora_injection_type == 'method1':\n",
    "    path = \"./llama_method1_injection\" #directory of method 1 trained LoRA injection model\n",
    "else:\n",
    "    path = \"./llama_method2_injection\" #directory of method 2 trained LoRA injection model\n",
    "\n",
    "#change cache_dir path to where you have kept meta-llama/Llama-2-13b-chat-hf\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    path,\n",
    "    load_in_4bit=True,\n",
    "    cache_dir=\"./llama_base_directory\",\n",
    "    device_map='auto',\n",
    "    use_cache=False\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9277d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.0,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    streamer=streamer\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c42c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_extract_lst(corrupt_lst):\n",
    "    lst_extract_prompt = f'''<s>[INST] <<SYS>>\n",
    " Example 1: Wrong Format: ['People's Republic of China', 'Laos', 'Thailand', 'India', 'Bangladesh']\"]. Correct Format: Answer: \"People's Republic of China\", \"Laos\", \"Thailand\", \"India\", \"Bangladesh\"] </s>\n",
    " Example 2: Wrong Format: ['Artibonite', 'Nord-Est Department', 'South Department', 'West Department', 'Centre Department', 'Grand'Anse Department', 'North Department']. Correct Format: Answer: \"Artibonite\", \"Nord-Est Department\", \"South Department\", \"West Department\", \"Centre Department\", \"Grand'Anse Department\", \"North Department\"] </s>\n",
    " Example 3: Wrong Format: ['book's and page's']. Correct Format: Answer: [\"book's and page's\"] </s>\n",
    " Your answer should only be a valid python list of string format. Do not give any explainations.\n",
    " <</SYS>>\n",
    "\n",
    " Use the examples to convert {corrupt_lst} into a correct python list. [/INST] Answer: '''\n",
    "\n",
    "    with llm.pipeline.model.disable_adapter():\n",
    "        ans = llm(lst_extract_prompt)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c772486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lst_regex(input_string):\n",
    "    extracted_list = []\n",
    "    pattern = r'\\[([^\\]]+)\\]'\n",
    "    matches = re.findall(pattern, input_string)\n",
    "    if matches:\n",
    "        if type(eval(matches[0])) is tuple:\n",
    "            extracted_list = list(eval(matches[0]))\n",
    "        else:\n",
    "            extracted_list = [str(eval(matches[0]))]\n",
    "    return extracted_list\n",
    "\n",
    "\n",
    "def extract_list_from_string(input_string):\n",
    "    try:\n",
    "        extracted_list = lst_regex(input_string)\n",
    "    except:\n",
    "        llm_lst = llm_extract_lst(input_string)\n",
    "        print(\"llm_lst\", llm_lst, type(llm_lst))\n",
    "        try:\n",
    "            extracted_list = lst_regex(llm_lst)\n",
    "        except:\n",
    "            extracted_list = []\n",
    "\n",
    "    return extracted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de97ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl_file_yieldable(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            yield json.loads(line)\n",
    "file_path = 'test_with_top3_context.jsonl'\n",
    "json_data_generator = read_jsonl_file_yieldable(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8377fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_top_context(sample):\n",
    "    return f'''<s>[INST] <<SYS>>\n",
    " You are a helpful, respectful and honest assistant. Your answers should be crisp, short and not repititive.\n",
    " Give valid wikipedia page titles in the answer. The answer should be in a python list of string format.\n",
    " If you dont know the answer from both the given context and your past knowledge, answer should just be a python empty list.\n",
    " <</SYS>>\n",
    " context: '{sample['top3context']}'\n",
    "    \n",
    " {question_prompts[question_prompts['Relation']==sample['Relation']]['PromptTemplate'].tolist()[0].replace('{subject_entity}',sample['SubjectEntity'])} [/INST]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsondata = []\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        single_sub_ent = next(json_data_generator)\n",
    "        formatted_sample = format_top_context(single_sub_ent)\n",
    "        res = llm(formatted_sample)\n",
    "        single_sub_ent['ObjectEntities'] = extract_list_from_string(res)\n",
    "        jsondata.append(single_sub_ent)\n",
    "    except StopIteration:\n",
    "        # Break out of the loop when there are no more values to yield\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred for record: {single_sub_ent}\")\n",
    "        print(f\"Error message: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b77ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('llama_stage1_inference.jsonl', 'w') as file:\n",
    "    json_string  = json.dumps(jsondata)\n",
    "    file.write(json_string + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
